# Simulation Optimization

Final project for CSE529 / AMS553. This project compares the efficacy of simulated-based combinatorial optimization algorithms mainly simulated annealing and genetic algorithms.

## Proposal

### Abstract

Simulated annealing is a simulated-based optimization algorithm that iteratively accepts worse solutions with some probability based on the current temperature. Genetic algorithms, on the other hand, explore a breadth of solutions, i.e. the population, in one generation and use evolution to produce the next generation. For our project proposal we intend to compare the depth-first approach of simulated annealing and the breadth-first approach of genetic algorithms on three key benchmark problems being travelling salesman problem, 0/1 knapsack and job scheduling. We compare both the quality of solutions after N iterations and the number of iterations needed to achieve a target fitness.

### Overview

In each iteration of simulated annealing a new candidate solution S' is generated by perturbing the previous solution S. We compute E=F(S)-F(S'), where F is some fitness function, and if E<0, we accept S', otherwise accept S' with probability e-E/T, where T is the temperature.

In each generation of genetic algorithm, we have P candidate solutions, where P is the population size. For each solution Si we compute a probability P(Si)=eF(Si)/T/SiSeF(S)/T based on the fitness using the softmax function. We then use a roulette wheel to select two parents and cross them over by randomly choosing cut points on either parent and combining the remaining solution from the other parent. Finally we use a random swap to introduce a mutation to either child. This is repeated P/2 times to produce P new children for the next generation.

To ensure consistency between SA and GA we need to ensure that solutions in SA are perturbed in the same way as the evolution process in GA. To generate a new solution in SA we first crossover the solution with itself by randomly choosing a cut point and swapping the two partitions. We also use a random swap to do a mutation. Overall there are three sources of randomness being selection, crossover and mutation. Finally, since GA explores P solutions in one generation, to ensure fairness we allow SA to run for P times more iterations.

### Experimental Setup

We compare SA and GA on three benchmark problems being travelling salesman problem, 0/1 knapsack and job scheduling. In the first experiment we will set the number of iterations as N and measure the quality of the final solution for various problem sizes. In the second experiment, we will use a deterministic approximation algorithm to compute a target fitness and measure the number of iterations needed to achieve the target fitness or better. For example, in the case of TSP we can compute a 2-OPT solution using MSTs. In both cases we normalize the number of iterations of SA to account for the number of generations in GA.

## Usage

This project uses uv to manage dependencies. Install uv [here](https://docs.astral.sh/uv/getting-started/installation/).

To install project dependencies run `uv sync`.

To run the project run `uv run`.
